{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import  r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  # Loading of the dataset via pandas\n",
    "  kc_data = pd.read_csv(\"../data/King_County_House_prices_dataset.csv\")\n",
    "  # We will drop this row\n",
    "  kc_data.drop(15856, axis=0, inplace=True)\n",
    "  return kc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We replace \"?\" with Nan\n",
    "\n",
    "class SqftColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X['sqft_basement'] = X['sqft_basement'].replace('?', np.NaN)\n",
    "        # And we change the dtype of the column \"sqft_basement\" to float\n",
    "        X['sqft_basement'] = X['sqft_basement'].astype(float)\n",
    "        X.eval('sqft_basement = sqft_living - sqft_above', inplace=True)\n",
    "        return X\n",
    "    \n",
    "class ViewColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We replace Nan values in \"view\" with the most frequent expression (0)\n",
    "        X['view'].fillna(0, inplace=True)\n",
    "        return X\n",
    "    \n",
    "class WaterFrontColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We replace Nan values in \"waterfront\" with the most frequent expression (0)\n",
    "        X.waterfront.fillna(0, inplace=True)\n",
    "        return X\n",
    "\n",
    "class LastKnownChangeColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We will create an empty list in which we will store values\n",
    "        last_known_change = []\n",
    "\n",
    "        # For each row in our data frame, we look at what is in the column \"yr_renovated\".\n",
    "        for idx, yr_re in X.yr_renovated.iteritems():\n",
    "            # if \"yr_renovated\" is 0 or contains no value, we store the year of construction of the house in our empty listes ab\n",
    "            if str(yr_re) == 'nan' or yr_re == 0.0:\n",
    "                last_known_change.append(X.yr_built[idx])\n",
    "            # if there is a value other than 0 in the column \"yr_renovated\", we transfer this value into our new list\n",
    "            else:\n",
    "                last_known_change.append(int(yr_re))\n",
    "        # We create a new column and take over the values of our previously created list\n",
    "        X['last_known_change'] = last_known_change\n",
    "        return X\n",
    "\n",
    "class DropExtraneousColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We delete the \"yr_renovated\" and \"yr_built\" columns\n",
    "        X.drop(\"yr_renovated\", axis=1, inplace=True)\n",
    "        X.drop(\"yr_built\", axis=1, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In feature engineering, we try to generate additional features (variables) that have a better explanatory power or a higher predictive power with respect to our target variable.\n",
    "\n",
    "In our case, we first want to adjust our target variable. Often people pay more attention to the price/performance ratio than to the price alone when making a purchase. In our example, we can achieve this by looking at the price per square foot of living space rather than the total price of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering transformers\n",
    "\n",
    "class SqFtPriceColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We create a new variable that gives us the price per square foot of living space\n",
    "        X['sqft_price'] = (X.price/(X.sqft_living + X.sqft_lot)).round(2)\n",
    "        return X\n",
    "\n",
    "class CentreOfWealthColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "      return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "      # Absolute difference of latitude between centre and property\n",
    "      X['delta_lat'] = np.absolute(47.62774- X['lat'])\n",
    "      # Absolute difference of longitude between centre and property\n",
    "      X['delta_long'] = np.absolute(-122.24194-X['long'])\n",
    "      # Distance between centre and property\n",
    "      X['center_distance']= ((X['delta_long']* np.cos(np.radians(47.6219)))**2 \n",
    "                                   + X['delta_lat']**2)**(1/2)*2*np.pi*6378/360\n",
    "      return X\n",
    "\n",
    "class WaterDistanceColumnTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "      return self\n",
    "    \n",
    "    # This function helps us to calculate the distance between the house overlooking the seafront and the other houses.\n",
    "    def dist(self, long, lat, ref_long, ref_lat):\n",
    "        '''dist computes the distance in km to a reference location. Input: long and lat of \n",
    "        the location of interest and ref_long and ref_lat as the long and lat of the reference location'''\n",
    "        delta_long = long - ref_long\n",
    "        delta_lat = lat - ref_lat\n",
    "        delta_long_corr = delta_long * np.cos(np.radians(ref_lat))\n",
    "        return ((delta_long_corr)**2 +(delta_lat)**2)**(1/2)*2*np.pi*6378/360\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        water_distance = []\n",
    "        water_list=  X.query('waterfront == 1')\n",
    "        \n",
    "        # For each row in our data frame we now calculate the distance to the seafront\n",
    "        for idx, lat in X.lat.iteritems():\n",
    "            ref_list = []\n",
    "            for x,y in zip(list(water_list.long), list(water_list.lat)):\n",
    "                ref_list.append(self.dist(X.long[idx], X.lat[idx],x,y).min())\n",
    "            water_distance.append(min(ref_list))\n",
    "        # wir erstellen eine neue Spalte und Ã¼bernehmen die Werte unserer vorher erstellten Liste\n",
    "        X['water_distance'] = water_distance\n",
    "        return X\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class PreprocessingSeattleHousing():\n",
    "    def __init__(self):\n",
    "        # Data cleaning Pipeline\n",
    "        self.data_cleaning_pipeline = Pipeline([\n",
    "            ('sqft_basement', SqftColumnTransformer()),\n",
    "            ('view', ViewColumnTransformer()),\n",
    "            ('waterfront', WaterFrontColumnTransformer()),\n",
    "            ('last_known_change', LastKnownChangeColumnTransformer()),\n",
    "            ('drop_extraneous_columns', DropExtraneousColumnsTransformer())\n",
    "        ])\n",
    "        # Feature Engineering\n",
    "        self.feature_enginneering = Pipeline([\n",
    "            ('sqft_price', SqFtPriceColumnTransformer()),\n",
    "            ('center_of_wealth', CentreOfWealthColumnsTransformer()),\n",
    "            ('water_distance', WaterDistanceColumnTransformer())\n",
    "        ])\n",
    "\n",
    "        self.preprocessor_pipe = Pipeline([\n",
    "            ('data_cleaning', self.data_cleaning_pipeline),\n",
    "            ('feature_enginneering', self.feature_enginneering)\n",
    "        ])\n",
    "\n",
    "    def preprocess_fit_transform(self, df):\n",
    "        return self.preprocessor_pipe.fit_transform(df)\n",
    "\n",
    "    def preprocess_transform(self, df):\n",
    "        return self.preprocessor_pipe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49        822039084\n",
      "230      8096000060\n",
      "246      2025069065\n",
      "264      2123039032\n",
      "300      3225069065\n",
      "            ...    \n",
      "19968    2025069140\n",
      "20309     518500480\n",
      "20751    8043700105\n",
      "21185     518500460\n",
      "21560    9253900271\n",
      "Name: id, Length: 146, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def train(dataset):\n",
    "  drop_lst = ['price', 'sqft_price', 'date', 'delta_lat', 'delta_long',]\n",
    "  # we would like to consider all variables except the ones mentioned above\n",
    "  all_features = [x for x in dataset.columns if x not in drop_lst]\n",
    "  # X contains all descriptive variables defined above\n",
    "  X = dataset[all_features]\n",
    "  # we define y (our dependent variable): we take the price\n",
    "  print(X.describe())\n",
    "  y = dataset['price']\n",
    "  # We separate our data into train and test data. In the process, 30 % of the data is used for the subsequent testing of the prognostic quality.\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "  # we can look at how much data is in each dataset\n",
    "  print(\"X_train (features for the model to learn from): \", X_train.shape)\n",
    "  print(\"y_train (labels for the model to learn from): \", y_train.shape)\n",
    "  print(\"X_test (features to test the model's accuracy against): \", X_test.shape)\n",
    "  print(\"y_test (labels to test the model's accuracy with): \", y_test.shape)\n",
    "  # If we look at the first 5 lines of our training data, we see that the index is no longer sorted, it has been shuffled.\n",
    "  X_train.head()\n",
    "\n",
    "def main():\n",
    "  df_dataset = load_data()\n",
    "\n",
    "  pipeline = PreprocessingSeattleHousing()\n",
    "  dataset = pipeline.preprocess_fit_transform(df_dataset)\n",
    "  train(dataset)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -20\n",
      "1     0\n",
      "Name: sqft_basement, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def example_housing_data():\n",
    "  csv_input = [\n",
    "    ['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n",
    "    ['7129300520', '10/13/2014', 221900.0, 3, 1.0, 1180, 5650, 1.0, None, 0.0, 3, 7, 1200, 0.0, 1955, 0.0, 98178, 47.5112, -122.257, 1340, 5650],\n",
    "    ['7129300521', '12/13/2019', 250900.0, 3, 1.0, 1180, 5650, 1.0, None, 0.0, 3, 7, 1180,'?', 1955, 0.0, 98178, 47.5112, -122.257, 1340, 5650]\n",
    "  ]\n",
    "  return pd.DataFrame(csv_input[1:], columns=csv_input[0])\n",
    "    \n",
    "def expected_results():\n",
    "  csv_results = [\n",
    "    ['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'],\n",
    "    ['7129300520', '10/13/2014', 221900.0, 3, 1.0, 1180, 5650, 1.0, None, 0.0, 3, 7, 1180, 0.0, 1955, 0.0, 98178, 47.5112, -122.257, 1340, 5650],\n",
    "    ['7129300521', '12/13/2019', 250900.0, 3, 1.0, 1180, 5650, 1.0, None, 0.0, 3, 7, 1180, 0.0, 1955, 0.0, 98178, 47.5112, -122.257, 1340, 5650]\n",
    "  ]\n",
    "\n",
    "result = example_housing_data().eval('sqft_basement = sqft_living - sqft_above')\n",
    "print(result['sqft_basement'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these preparations, we now come to modelling. \n",
    "For this we will continue to use the scikit-learn library, in which many different algorithms are implemented.\n",
    "The procedure is always the same:\n",
    "\n",
    "- we import the algorithm from scikit-learn which we want to use.\n",
    "\n",
    "- we determine the model, often there are additional hyperparameters we have to specify\n",
    "- we determine which variables to pass to the model\n",
    "- we train the model (we call the method `.fit(X_train, y_train)` on our model)\n",
    "- we test the model with our test data and get the adjusted R^2 as metric (we call the method `.score(X_test, y_test)` on our trained model and clean up the score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "model_lin_reg = LinearRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we determine which variables we pass to our model to determine the price of the houses.\n",
    "The simplest model calculates the price using only one variable: for example, the \"grade\" of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine which variables we pass to the model\n",
    "variables = ['grade',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model\n",
    "model_lin_reg.fit(X_train[variables], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "print('adj. R^2:', (1-(1-model_lin_reg.score(X_test[variables], y_test))*(X_test.shape[0]- 1)/(X_test.shape[0]-len(variables)-1)).round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R^2 indicates the percentage of variance of the target variable (price per square foot) explained by the model. Adjusted RÂ² is a modified version of RÂ² that has been adjusted with the number of explanatory variables. It penalises the addition of unnecessary variables and allows comparison of regression models with different numbers of explanatory variables.\n",
    "The value 1 means 100 % of the variance of the target variable could be explained by the model. The value 0 means 0 % of the variance of the target variable could be explained by the model. \n",
    "This means for our case: The variable \"grade\" can explain 43 % of the variance in the price per square foot of the houses in our test set.\n",
    "Perhaps more variables could explain more variance. \n",
    "We can look again at what variables we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the variables in the data set\n",
    "X_train.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use all these variables to predict the price per square foot.\n",
    "Maybe the age of the house could also play a big influence.\n",
    "Therefore, we will now try a new linear regression with these 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "model_lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine which variables we pass to the model\n",
    "variables = ['grade','last_known_change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train the model\n",
    "model_lin_reg.fit(X_train[variables], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "print('adj. R^2:', (1-(1-model_lin_reg.score(X_test[variables], y_test))*(X_test.shape[0]- 1)/(X_test.shape[0]-len(variables)-1)).round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with this additional variable, 48% of the variance in the price per square foot could be explained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only looked at the linear relationships between the variables and the price. However, it is possible that the relationship is not linear, but rather quadratic. \n",
    "We can easily extend our model by squaring our variables. Thus, instead of:\n",
    "\n",
    "$price = b*(grade) + c$\n",
    "\n",
    "we can also use \n",
    "\n",
    "$price=a*(grade)^{2}+b*(grade)+c$\n",
    "\n",
    "can be obtained.\n",
    "This is a type of feature engineering. We will apply it to our complete data set and see if we can improve our model even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create only polynomial variables of second order (^2)\n",
    "poly = PolynomialFeatures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the Train and Test data\n",
    "X_train_poly = X_train.copy()\n",
    "X_test_poly = X_test.copy()\n",
    "\n",
    "# drop the id column\n",
    "X_train_poly = X_train_poly.drop(columns=['id'])\n",
    "X_test_poly = X_test_poly.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create new variables by calling poly\n",
    "X_train_sq = poly.fit_transform(X_train_poly)\n",
    "\n",
    "# We have to do the same for our test data, of course\n",
    "X_test_sq = poly.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "model_lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also train the model with squared variables\n",
    "model_lin_reg.fit(X_train_sq, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "print('adjusted R^2:', (1-(1-model_lin_reg.score(X_test_sq, y_test))*(X_test_sq.shape[0]- 1)/(X_test_sq.shape[0]-X_test_sq.shape[1]-1)).round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the additional squared variables, we were able to improve our result a bit more.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the adjusted R^2 value we have a possibility to evaluate the quality of our model, but it may be worthwhile to have a look at the real errors of the model graphically. This may help to identify systematic errors.\n",
    "For ease of interpretation, we choose the percentage price difference between our forecast and the true values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a few outliers here. We can take a closer look at the highest one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "# In order to better analyse the errors of our model, we create a new dataframe with the\n",
    "# columns \"price\" (the real price), as well as the latitudes and longitudes\n",
    "y_predictions = model_lin_reg.predict(X_test_sq)\n",
    "df_error = pd.DataFrame(y_test)\n",
    "df_error['latitude'] = X_test['lat']\n",
    "df_error['longitude'] = X_test['long']\n",
    "df_error['id'] = X_test['id']\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add the predicted price as a column as well, we must first reset the index\n",
    "df_error.reset_index(inplace=True, drop=True)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can also add the predicted price as a column and calculate the difference\n",
    "df_error['price_prediction'] = y_predictions.round(2)\n",
    "df_error['price_difference'] = (df_error['price_prediction'] - df_error['price']).round(2)\n",
    "df_error['price_difference_procent'] = ((df_error['price_difference']/df_error['price'])*100).round(2)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df_error,\n",
    "                        lat=\"latitude\",\n",
    "                        lon=\"longitude\",\n",
    "                        hover_data=[\"price\", \"price_prediction\", 'id'],\n",
    "                        color='price_difference_procent',\n",
    "                        color_continuous_scale=['green', 'yellow', 'red'],\n",
    "                        zoom=7.7,\n",
    "                        height=400)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error[df_error['price_difference_procent']==df_error['price_difference_procent'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test['id']==9272202260]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to take a closer look at these houses. King County also provides very good information on this. On [this page](https://localscape.property/#kingcountyassessor/My-Property) you can search for houses by their ID and get both the neighbourhood on a map and a picture of the house.\n",
    "In the field at the top left, change the selection \"Address\" to \"Parcel ID\" and add the \"id\" of our outlier.\n",
    "\n",
    "Under \"Basic Property Characteristics\" and on the map under \"KC Aerial Images\" we see that there is no longer a house on this property. Therefore, our data is misleading and our model estimates a much higher price.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation and hyperparameter tuning of linear regression\n",
    "\n",
    "In addition to our variables, we have also passed the squared variables to our last linear model. So we have passed a lot of variables to our model. Some may have no effect on the price at all. However, models try to extract some information from all variables. This leads to random variance in the data also being recognised as a pattern. This phenomenon is called \"overfitting\" the model to the data.\n",
    "For each algorithm there are ways to prevent this overfitting.\n",
    "\n",
    "In the case of linear regression, we force the model not to use variables for forecasting. We \"regularise\" the model. But instead of us telling the model which variables not to use, we let the model learn which variables offer the least added value and remove those variables (in the case of linear regression, variables are no longer considered if the learned coefficient (b) is zero). How much we regularise is up to us.\n",
    "[Here](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net) you can find information on the ElasticNet model used.\n",
    "To find out how much we should regularize our model, we can test different values for the regularzation parameters and see which will give us the best model. For this purpose we can use [GridSearch](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) with [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify which hyperparameters we want to change:\n",
    "# alpha: specifies how much we regularise:\n",
    "param_grid = {'alpha':[0.1,.5,1,5,10,],\n",
    "             'l1_ratio': [1,0.5,0]}\n",
    "\n",
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "elastic = ElasticNet(max_iter=50000, tol=0.2)\n",
    "\n",
    "# Passing the model to a so-called parameter grid with 5-fold cross-validation\n",
    "elastic_cv= GridSearchCV(elastic,param_grid,cv=5, verbose=True,n_jobs=-1)\n",
    "\n",
    "# We train the model and optimise it via GridSearch\n",
    "elastic_cv.fit(X_train_sq,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the best parameters found by the GridSearch.\n",
    "elastic_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model with the optimal hyperparameters\n",
    "elastic = ElasticNet(max_iter=50000, tol=0.2,**elastic_cv.best_params_)\n",
    "\n",
    "elastic.fit(X_train_sq, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "adj_r2 = (1-(1-elastic.score(X_test_sq, y_test))*(X_test_sq.shape[0]- 1)/(X_test_sq.shape[0]-(X_test_sq.shape[1]-sum(elastic.coef_== 0))-1)).round(2)\n",
    "print('adjusted R^2:',adj_r2 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to further improve our model through hyperparameter tuning and regularisation. \n",
    "As mentioned above, regularisation eliminates variables from the forecast. This is done by giving the coefficients of the linear regression a value of zero. With the following code we can look at the learned coefficients (here only a section of the first 5 coefficients) and see that for the first variable a coefficient of zero was calculated. This variable was therefore removed by our regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the first 5 learned coefficients of linear regression\n",
    "elastic.coef_[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "# In order to better analyse the errors of our model, we create a new dataframe with the\n",
    "# columns \"price\" (the real price), as well as the latitudes and longitudes\n",
    "y_predictions = elastic.predict(X_test_sq)\n",
    "df_error = pd.DataFrame(y_test)\n",
    "df_error['latitude'] = X_test['lat']\n",
    "df_error['longitude'] = X_test['long']\n",
    "df_error['id'] = X_test['id']\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add the predicted price as a column as well, we must first reset the index\n",
    "df_error.reset_index(inplace=True, drop=True)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can also add the predicted price as a column and calculate the difference \n",
    "df_error['price_prediction'] = y_predictions.round(2)\n",
    "df_error['price_difference'] = (df_error['price_prediction'] - df_error['price']).round(2)\n",
    "df_error['price_difference_procent'] = ((df_error['price_difference']/df_error['price'])*100).round(2)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df_error,\n",
    "                        lat=\"latitude\",\n",
    "                        lon=\"longitude\",\n",
    "                        hover_data=[\"price\", \"price_prediction\", 'id'],\n",
    "                        color='price_difference_procent',\n",
    "                        color_continuous_scale=['green', 'yellow', 'red'],\n",
    "                        zoom=7.7,\n",
    "                        height=400)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error[df_error['price_difference_procent']==df_error['price_difference_procent'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test['id']==5111400086]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this house. King County also provides very good information about it. On [this page](https://localscape.property/#kingcountyassessor/My-Property) you can search for houses by their ID and get both the neighbourhood on a map and a picture of the house.\n",
    "In the box at the top left, they change the \"Address\" selection to \"Parcel ID\" and add the \"id\" of our outlier there.\n",
    "\n",
    "This house sold in 2018 for a price of  295,000 USD. But King County appraisers also assessed a higher price in 2014: under \"Historical Value\" they see that in 2014 the \"Total Assessed Value\" was 212,000 USD. So this house sold for about half the appraised price!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "We have now trained a model that can predict the price of a house in King County. We can now save this model using [skops](https://skops.readthedocs.io/en/stable/modules/classes.html#module-skops.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skops.io as sio\n",
    "\n",
    "with open('model/model.bin', 'wb') as f_out:\n",
    "    sio.dump(elastic, f_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Despite the outliers, we succeeded in creating a model that predicts prices with an accuracy of 76%. We found that the creation of new variables, but also the squaring of these variables and the regularisation of the model play an important role in the quality of the prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb7e7bd19e6b082abfa5136f7b755936d5cc9a5dcc8aebb1a765dc8cdd06fcd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
