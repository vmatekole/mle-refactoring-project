{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for EDA\n",
    "\n",
    "In this notebook we will go through all the important steps of the data science life cycle using an example. You already got to know the first sections of this notebook in the last workshop in Python. This time we will go a few steps further and also look at machine learning models and evaluate their results.\n",
    "\n",
    "As a little reminder, here is the Data Science Life Cycle again.\n",
    "\n",
    "\n",
    "<center><img src=\"./images/DS_LC.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "The goal is to implement the essential steps of a supervised-learning work process using the Python library scikit-learn.\n",
    "We will use the following methods: Feature Engineering, train-test-split, train different models, hyper-parameter optimisation using grid-search and cross-validation, evaluation of model goodness.\n",
    "\n",
    "To illustrate all these steps, let us imagine the following (fictitious) scenario: \n",
    "\n",
    "**You have found your dream job as a Data Scientist in a very innovative and exciting company. They have received a very interesting job offer to you which you cannot decline. This company is located near Seattle and, therefore, you are now looking for a suitable property in the vicinity of Seattle.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/seattle_skyline.jpg)\n",
    "(<span>Photo by <a href=\"https://unsplash.com/@phoebezzf?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Zhifei Zhou</a> on <a href=\"https://unsplash.com/s/photos/seattle-skyline?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></span>)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a person interested in data, you don't want to leave this house hunt to chance and make your decision based on data after good research. To achieve this, we try to predict home prices around Seattle so that you have a good estimate of the true value of the property for negotiation purposes.**\n",
    "\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "Seattle is located in King County, which is shown on the map. To avoid a long drive to your new job in Seattle, limit your property search to this area.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/king_county_districts.jpeg)\n",
    "\n",
    "(Photo from King County [imap](https://gismaps.kingcounty.gov/iMap/))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we are using a dataset, which contains data on all property sales in King County from May 2014 to May 2015.\n",
    "\n",
    "First, it is important to find out what information is contained in the dataset. \n",
    "Even though we will only see the real information later, the explanations of the individual variables are listed here. \n",
    "\n",
    "\n",
    "|**Column Names**   |  **Description** |\n",
    "|---                |---:               |\n",
    "| id                | unique identified for a house  | \n",
    "| date              | date the house was sold  |\n",
    "| price             | price for each house  |\n",
    "| bedrooms          | number of bedrooms  |\n",
    "| bathrooms         | number of bathrooms,  where .5 stands for a room with a toilet but no shower  |\n",
    "| sqft_living       | square footage of the home  |\n",
    "| sqft_lot          | square footage of the lot   |\n",
    "| floors            | number of floors  |\n",
    "| waterfront        | Has the house on the water view? 0-No 1-Yes  |\n",
    "| view              | An index from 0 to 4 indicating how good the survey of the property was  |\n",
    "| condition         | An index from 1 to 5 for the condition of the flat,1 - lowest, 5 - highest  |\n",
    "| grade             |  An index of 1 to 13, where 1-3 lags behind construction and design of buildings, 7 shows an average level of construction and design and 11-13 shows a high level of quality of construction and design. |\n",
    "| sqft_basement     | square footage of the basement  |\n",
    "| yr_built          | The year in which the house was originally built |\n",
    "| yr_renovated      | The year of the last renovation of the house   |\n",
    "| zipcode           | In which zipcode area is the house located  |\n",
    "| lat               | latitude  |\n",
    "| long              | logitude  |\n",
    "| sqft_living15     | The square footage of interior housing living space for the nearest 15 neighbors  |\n",
    "| sqft_lot15        | The square footage of the land lots of the nearest 15 neighbors  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries\n",
    "\n",
    "\n",
    "First we will import again all the libraries we need for our future work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import  r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of the dataset via pandas\n",
    "kc_data = pd.read_csv(\"data/King_County_House_prices_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the first 5 lines of our data set.\n",
    "# We want to make sure that the data has been read in correctly.\n",
    "kc_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation \n",
    "\n",
    "After we have successfully loaded the data, we need a good overview of the quality of the data. \n",
    "In doing so, we would like to answer a few questions:\n",
    "- How many house sales are in our data set?\n",
    "- What format is the data in (int=number, float= decimal, object=text)?\n",
    "- Are there any missing values?\n",
    "- How many values are there for the individual variables?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  How many house sales are in our data set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kc_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kc_data\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kc_data' is not defined"
     ]
    }
   ],
   "source": [
    "kc_data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row contains the details of one house sale:\n",
    "\n",
    "21597 house sales are included in our dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What format is the data in (int=number, float= decimal number, object=text)?\n",
    "Which formats are surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kc_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kc_data\u001b[39m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kc_data' is not defined"
     ]
    }
   ],
   "source": [
    "kc_data.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 8 columns with decimal numbers, 11 columns with integers (int) and 2 columns with object (text).\n",
    "We expect all data on the size of the area, the number of rooms, the indices and the information on the location to be recognised as numbers.\n",
    "The date is recognised as Object(Text) as well as the information about the area of the basement (sqft_basement).\n",
    "This is surprising. A little further up, when we looked at the first 10 rows of our table, you can see why the values are not recognised as numbers: there is a \"?\" mixed in.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Are there any missing values?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values in the columns \"waterfront\", \"view\" and \"yr_renovated\". The most missing values are in \"yr_renovated\".\n",
    "    \n",
    "Further on we have to think about how to deal with these missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many values are there for the individual variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data.nunique()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting findings:\n",
    "1. we have 21420 different house IDs: this means that up to 177 houses may have been sold twice. \n",
    "    \n",
    "2. we have 3622 expressions for the price of sold houses: many houses were sold for the same price.\n",
    "    \n",
    "3. although the \"grade\" is an index from 1-13, we have only 11 different values included.\n",
    "    \n",
    "4. the houses in our dataset were built in 116 different years."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can display the distribution of the individual columns. Here, too, you are sure to discover a few interesting insights.\n",
    "\n",
    "Small explanation of the scattering measures given:\n",
    "- **count**: Indication of how many values are present in the columns (NaNs/missing values are not counted).\n",
    "- **mean**: Mean value of the data\n",
    "- **std**: standard deviation of the data\n",
    "- **min**: the smallest value in the data set\n",
    "- **25%**: 25% of the data are below this value\n",
    "- **50%**: 50% of the data are below this value. This value is called the median.\n",
    "- **75 %**: 75 % of the data are below this value.\n",
    "- **max**: the largest expression in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data.describe().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting findings:\n",
    "\n",
    "1. on average, the houses in this data set cost \\$ 540 296. The most expensive house sold for 7 700 000 \\$, the cheapest for 78 000 \\$.\n",
    "\n",
    "2. 50% of the houses have 3 or less bedrooms. The house with the most bedrooms has 33! (Is that possible?)\n",
    "\n",
    "    \n",
    "3. on average, each house has 2.1 bathrooms per bedroom.\n",
    "\n",
    "4. 75% of houses have 2 or fewer floors.\n",
    "\n",
    "5. only 7.5% of the houses have a view of the seafront (since Waterfront can only take the values 0 and 1, the mean can be interpreted as the percentage of houses with a view of the seafront).\n",
    "\n",
    "6. the oldest house in our dataset is from 1900, the newest house from 2015."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can find many more insights from the table above, but we have already discovered a few inconsistencies in any case. We should deal with these in Data Cleaning:\n",
    "- Missing values: how do we want to deal with them?\n",
    "- \"?\" in the column \"sqft_basement\" (change format from object to integer)\n",
    "- the entry with 33 bedrooms\n",
    "\n",
    "All other distributions of our data can be much better visualised later in the exploratory data analysis with charts.\n",
    "First we take care of the inconsistencies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**33 bedrooms**\n",
    "\n",
    "If we look at the maximum number of bedrooms, we see a house with 33 bedrooms but only 2 bathrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the line in which the condition \"bedrooms = 33\" is present.\n",
    "kc_data.query('bedrooms == 33')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ratio of 33 bedrooms to 2 bathrooms in 1620 square feet sounds very unlikely, we are removing this house from the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop this row\n",
    "kc_data.drop(15856, axis=0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"?\" in \"sqft_basement**\n",
    "\n",
    "We have seen in the output of .info() that \"sqft_basement\" is present as an object and not as a number. There is a \"?\" in this column, although actually only numbers should be in this column. We now replace this with a NaN and calculate all missing values from \"sqft_living - sqft_above\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace \"?\" with Nan\n",
    "kc_data['sqft_basement'] = kc_data['sqft_basement'].replace('?', np.NaN)\n",
    "# And we change the dtype of the column \"sqft_basement\" to float\n",
    "kc_data['sqft_basement'] = kc_data['sqft_basement'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are calculating the \"sqft_basement\" by substracting sqft_above of sqft_living\n",
    "kc_data.eval('sqft_basement = sqft_living - sqft_above', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missings Values**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summation of the missing values and calculation of the missing values as a percentage\n",
    "missing_values = pd.DataFrame(kc_data.isnull().sum(),columns=['count'])\n",
    "missing_values['percentage'] = (missing_values['count']/kc_data.shape[0]*100).round(2)\n",
    "missing_values.query('count != 0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the missing data, we see that out of 21 variables, 3 variables have missing values (missing values at \"sqft_basement\" have already been replaced):\n",
    "\n",
    "- waterfront has 2376 missing values (11 %)\n",
    "- view has 63 missing values (0.29 %)\n",
    "- yr_renovated has 3842 missing values (17.79 %)\n",
    "\n",
    "\n",
    "The column 'view' has only 0.29 % missing values and has 19422 times out of 21597 the value 0, so we decide to replace the missing values with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display how often the different values of the variable occur.\n",
    "kc_data['view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace Nan values in \"view\" with the most frequent expression (0)\n",
    "kc_data['view'].fillna(0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the column \"waterfront\" we see a similar distribution, also here we replace the NaNs by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display how often the different values of the variable occur.\n",
    "kc_data.waterfront.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace Nan values in \"waterfront\" with the most frequent expression (0)\n",
    "kc_data.waterfront.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We again look at the missing data\n",
    "missing_values = pd.DataFrame(kc_data.isnull().sum(),columns=['count'])\n",
    "missing_values['percentage'] = missing_values['count']/kc_data.shape[0]*100\n",
    "missing_values.query('count != 0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 17.8% of the data is missing from \"yr_renovated\", we create a new column \"last_known_change\" from \"yr_built\" and \"yr_renovated\". And remove the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an empty list in which we will store values\n",
    "last_known_change = []\n",
    "\n",
    "# For each row in our data frame, we look at what is in the column \"yr_renovated\".\n",
    "for idx, yr_re in kc_data.yr_renovated.iteritems():\n",
    "    # if \"yr_renovated\" is 0 or contains no value, we store the year of construction of the house in our empty listes ab\n",
    "    if str(yr_re) == 'nan' or yr_re == 0.0:\n",
    "        last_known_change.append(kc_data.yr_built[idx])\n",
    "    # if there is a value other than 0 in the column \"yr_renovated\", we transfer this value into our new list\n",
    "    else:\n",
    "        last_known_change.append(int(yr_re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new column and take over the values of our previously created list\n",
    "kc_data['last_known_change'] = last_known_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We delete the \"yr_renovated\" and \"yr_built\" columns\n",
    "kc_data.drop(\"yr_renovated\", axis=1, inplace=True)\n",
    "kc_data.drop(\"yr_built\", axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with the data cleaning. With the Pandas commands, especially with `.describe()`, we have already obtained a quick overview of the distribution of the individual variables. In the explorative data analysis, however, we also want to display the distribution of the data graphically."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First of all, let's look at the distribution of the individual variables in our data set. This works very well with histograms. Here we show the frequency distribution of the individual variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of certain variables we want a closer look at\n",
    "columns_histogram = ['price', 'bathrooms', 'bedrooms', 'floors', 'grade', 'last_known_change', 'sqft_living', 'sqft_lot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of Histograms\n",
    "kc_data[columns_histogram].hist(bins=50, figsize = (20,15))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the histograms, you can quickly see how the data is distributed, but also whether it is categorical.\n",
    "Findings:\n",
    "- Over 5000 houses have 2.5 bathrooms (0.5 bathroom in America is a room with only a toilet). This makes 2.5 bathrooms the most common value in our dataset.\n",
    "- most houses in our dataset have between 2 and 5 bedrooms, with 3 bedrooms being the most common.\n",
    "- most of the houses in our dataset have only one floor.\n",
    "- most of the houses in our dataset have a rating between 6 and 10 (scale 1-13).\n",
    "- the oldest house in our dataset dates from 1900. very few houses were built or renovated between 1930 and 1940. After that, more houses were built or renovated.\n",
    "- In terms of price, we see a right-skewed distribution. In addition, there are very extreme values here, which is why the position of the most frequent values is very much to the left.\n",
    "- We also see a slight right-skewed distribution for living space.\n",
    "- The extreme values for the land areas distort the histogram extremely. In order to be able to recognise a true distribution of the values, extreme values can be removed here.\n",
    "\n",
    "Another form of representation of the distribution of the variables is a box plot. Here the individual distribution measures, which we have already got to know with `kc_data.describe()`, are displayed graphically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of the Box-Plots\n",
    "fig = px.box(kc_data, y=\"price\", labels={\"price\": \"House Price in $\"})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some outliers in our data set here. Outliers can strongly influence the quality of models. Therefore, it is important to think about why outliers occur. \n",
    "In the case of our house with 33 bedrooms, we were suspicious of this information, possibly someone made a typing error there and the house only has 3 bedrooms. Since it was only an isolated case, we deleted this line completely. \n",
    "However, many houses with very high prices appear here. Of course, these very expensive houses are a rarity, but they can be explained quite logically: there are a lot of very rich people living in Seattle who can afford such a property. For this reason, we will not remove these outliers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it is very interesting to look at the correlation between the individual variables. This \"pair plot\" is suitable for a quick overview. Here, the individual variables (e.g. first row: price) are shown in a scatter plot over the other variables. On the diagonal (top left corner: price over price) the histogram of the individual variables is shown.\n",
    "\n",
    "Interesting findings here could be correlations between two variables. That means: how do two variables influence each other? For example, how does the price change when the living space increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of Scatterplots\n",
    "sns.pairplot(kc_data[columns_histogram]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different representation of the correlation (linear relationship) is achieved through a heat map. Here, the correlation of each variable with every other variable is determined. It shows how strong the linear correlation is. Extremely strong colours indicate a strong correlation. Red indicates a positive correlation (e.g. with increasing living space the price increases), blue indicates a negative correlation (e.g. with increasing price the distance to the centre decreases).\n",
    "\n",
    "It is very interesting to see how much the individual variables influence the price. From this, one can deduce which variables are well suited to predict our target variable in a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the Pearson correlation coefficients\n",
    "mask = np.triu(kc_data.corr())\n",
    "plt.figure(figsize = (20,15))\n",
    "ax = sns.heatmap(round(kc_data.corr(), 2)\n",
    "                 ,annot=True\n",
    "                 ,mask=mask\n",
    "                 ,cmap='RdBu_r')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In feature engineering, we try to generate additional features (variables) that have a better explanatory power or a higher predictive power with respect to our target variable.\n",
    "\n",
    "In our case, we first want to adjust our target variable. Often people pay more attention to the price/performance ratio than to the price alone when making a purchase. In our example, we can achieve this by looking at the price per square foot of living space rather than the total price of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new variable that gives us the price per square foot of living space\n",
    "kc_data['sqft_price'] = (kc_data.price/(kc_data.sqft_living + kc_data.sqft_lot)).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance to the centre of wealth**.\n",
    "\n",
    "One of the richest people in the world, Bill Gates; has a residence in Medina right on Lake Washington. According to Wikipedia, the estate has a total estimated value of US$ 147.5 million.\n",
    "\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Residence_of_Bill_Gates.jpg#/media/File:Residence_of_Bill_Gates.jpg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Residence_of_Bill_Gates.jpg/1200px-Residence_of_Bill_Gates.jpg\" alt=\"Bill Gates' mansion annat\"></p>\n",
    "\n",
    "Also interesting for the price of a house in King County might be how far it is from the centre of wealth. To do this, we first calculate the absolute difference in latitude and longitude of each house to Bill Gates' residence and then calculate the physical distance between each house and the \"centre\" above in kilometres (this requires renormalising longitude by a factor cos(lat) to correct for the fact that the spatial distance of one degree of longitude depends on latitude (6378 is the radius of the earth in kilometres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data[kc_data['sqft_price']==kc_data['sqft_price'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_data[kc_data['price']==kc_data['price'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute difference of latitude between centre and property\n",
    "kc_data['delta_lat'] = np.absolute(47.62774- kc_data['lat'])\n",
    "# Absolute difference of longitude between centre and property\n",
    "kc_data['delta_long'] = np.absolute(-122.24194-kc_data['long'])\n",
    "# Distance between centre and property\n",
    "kc_data['center_distance']= ((kc_data['delta_long']* np.cos(np.radians(47.6219)))**2 \n",
    "                                   + kc_data['delta_lat']**2)**(1/2)*2*np.pi*6378/360"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at whether our assumption is correct and whether properties close to the city centre are more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Price over distance to centre\n",
    "sns.relplot(y=\"price\", x=\"center_distance\", data=kc_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: price per square foot of living space versus distance to the centre\n",
    "sns.relplot(y=\"sqft_price\", x=\"center_distance\", data=kc_data);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagrams illustrate our hypothesis: the price decreases with increasing distance from the centre. The price/performance ratio improves with increasing distance from the centre: the price per square foot decreases.\n",
    "\n",
    "We can view all the existing houses in our dataset once on the map of King County. The colour indicates how close the house is to the centre of wealth.\n",
    "The size of the dots corresponds to the height of the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter_mapbox(kc_data,\n",
    "                        lat=\"lat\",\n",
    "                        lon=\"long\",\n",
    "                        hover_name=\"id\",\n",
    "                        hover_data=[\"sqft_price\", \"sqft_living\", \"zipcode\", 'floors'],\n",
    "                        size='sqft_price',\n",
    "                        color='center_distance',\n",
    "                        color_continuous_scale=['green', 'yellow', 'red'],\n",
    "                        color_discrete_sequence=[\"fuchsia\"],\n",
    "                        zoom=7.7,\n",
    "                        height=400)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Distance to the beach promenade**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we try to construct a measure of water proximity. For this purpose we extract the list of houses with waterfront = 1 and call it water_list. Then we calculate for each house the minimum distance to a member of the water_list. We call this minimum distance water_distance. This is not entirely accurate, since the houses in water_list do not make up all the houses near the coast, but we have no better data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to calculate the distance between the house overlooking the seafront and the other houses.\n",
    "def dist(long, lat, ref_long, ref_lat):\n",
    "    '''dist computes the distance in km to a reference location. Input: long and lat of \n",
    "    the location of interest and ref_long and ref_lat as the long and lat of the reference location'''\n",
    "    delta_long = long - ref_long\n",
    "    delta_lat = lat - ref_lat\n",
    "    delta_long_corr = delta_long * np.cos(np.radians(ref_lat))\n",
    "    return ((delta_long_corr)**2 +(delta_lat)**2)**(1/2)*2*np.pi*6378/360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All houses with \"waterfront\" are added to the list\n",
    "water_list= kc_data.query('waterfront == 1')\n",
    "water_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_distance = []\n",
    "# For each row in our data frame we now calculate the distance to the seafront\n",
    "for idx, lat in kc_data.lat.iteritems():\n",
    "    ref_list = []\n",
    "    for x,y in zip(list(water_list.long), list(water_list.lat)):\n",
    "        ref_list.append(dist(kc_data.long[idx], kc_data.lat[idx],x,y).min())\n",
    "    water_distance.append(min(ref_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir erstellen eine neue Spalte und übernehmen die Werte unserer vorher erstellten Liste\n",
    "kc_data['water_distance'] = water_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a new column and take over the values of our previously created list\n",
    "kc_data.describe().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new variables have been added to our table at the very end. On average, the houses in our dataset are 17 km from the centre and almost 19 km from a house with a water view. The house with the greatest distance to the centre is about 70 km away.\n",
    "\n",
    "Again, we would like to test our hypothesis whether the proximity to the water has an influence on the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"price\", x=\"water_distance\", data=kc_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"sqft_price\", x=\"water_distance\", data=kc_data);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagrams illustrate our hypothesis: the price decreases with increasing distance to the water. The price/performance ratio improves with increasing distance to the water: the price per square foot decreases.\n",
    "\n",
    "On the map we can look again at whether we have calculated our variable correctly. We see that houses close to the water have low water_distances (green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(kc_data.query('water_distance <= 5'),\n",
    "                        lat=\"lat\",\n",
    "                        lon=\"long\",\n",
    "                        hover_name=\"id\",\n",
    "                        hover_data=[\"sqft_price\", \"sqft_living\", \"zipcode\", 'floors'],\n",
    "                        size='sqft_price',\n",
    "                        color='water_distance',\n",
    "                        color_continuous_scale=['green', 'yellow', 'red'],\n",
    "                        color_discrete_sequence=[\"fuchsia\"],\n",
    "                        zoom=8.4,\n",
    "                        height=400)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "\n",
    "Now that we have looked at the data in more depth, we can start to forecast possible new houses.\n",
    "\n",
    "Our goal is to determine the price for which new houses will sell.\n",
    "\n",
    "In this scenario, we realise that we do not yet know the true sales price until the house has been successfully sold. We can only test our model if we have such new and unknown data. However, we do not have this data at the moment.\n",
    "\n",
    "Therefore, we only work with part of the data to train our model and keep another part as test data. We consider this test set as new house sales that we want to estimate with our model. We can then compare the results of the model with the true sales prices and determine how well our model determines the true sales prices.\n",
    "To separate the dataset into training and testing data, we use a feature of Scikit-Learn: Train-Test-Split. Here we specify what our independent variables are (X) and what our predicted dependent variable is (y). We also specify what percentage of the data the test set should be. Another important parameter we submit is called \" random state\". With this parameter, our data is first shuffled before it is split into the train and the test set. This is important because the order of the data should not affect the prices of the houses (imagine if we sorted the data by price and then cut the bottom 30% for our test set, this would mean that our model would never have seen houses in that price category and would therefore be very difficult to apply to the new data). By assigning a number to the random state, we ensure that our data is mixed but always identically separated so that the results are reproducible.\n",
    "\n",
    "Before we make this important split, we remove the columns in our table that have no predictive value. It is also important to store our dependent variable (y) separately and remove any variables that could cause data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to remove these columns because of Data Leakage: price or because they do not provide prognostic information.\n",
    "drop_lst = ['price', 'sqft_price', 'date', 'delta_lat', 'delta_long',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would like to consider all variables except the ones mentioned above\n",
    "all_features = [x for x in kc_data.columns if x not in drop_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X contains all descriptive variables defined above\n",
    "X = kc_data[all_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define y (our dependent variable): we take the price\n",
    "y = kc_data.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We separate our data into train and test data. In the process, 30 % of the data is used for the subsequent testing of the prognostic quality.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at how much data is in each dataset\n",
    "print(\"X_train (features for the model to learn from): \", X_train.shape)\n",
    "print(\"y_train (labels for the model to learn from): \", y_train.shape)\n",
    "print(\"X_test (features to test the model's accuracy against): \", X_test.shape)\n",
    "print(\"y_test (labels to test the model's accuracy with): \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at the first 5 lines of our training data, we see that the index is no longer sorted, it has been shuffled.\n",
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these preparations, we now come to modelling. \n",
    "For this we will continue to use the scikit-learn library, in which many different algorithms are implemented.\n",
    "The procedure is always the same:\n",
    "\n",
    "- we import the algorithm from scikit-learn which we want to use.\n",
    "\n",
    "- we determine the model, often there are additional hyperparameters we have to specify\n",
    "- we determine which variables to pass to the model\n",
    "- we train the model (we call the method `.fit(X_train, y_train)` on our model)\n",
    "- we test the model with our test data and get the adjusted R^2 as metric (we call the method `.score(X_test, y_test)` on our trained model and clean up the score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "model_lin_reg = LinearRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we determine which variables we pass to our model to determine the price of the houses.\n",
    "The simplest model calculates the price using only one variable: for example, the \"grade\" of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine which variables we pass to the model\n",
    "variables = ['grade',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the model\n",
    "model_lin_reg.fit(X_train[variables], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "print('adj. R^2:', (1-(1-model_lin_reg.score(X_test[variables], y_test))*(X_test.shape[0]- 1)/(X_test.shape[0]-len(variables)-1)).round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted R^2 indicates the percentage of variance of the target variable (price per square foot) explained by the model. Adjusted R² is a modified version of R² that has been adjusted with the number of explanatory variables. It penalises the addition of unnecessary variables and allows comparison of regression models with different numbers of explanatory variables.\n",
    "The value 1 means 100 % of the variance of the target variable could be explained by the model. The value 0 means 0 % of the variance of the target variable could be explained by the model. \n",
    "This means for our case: The variable \"grade\" can explain 43 % of the variance in the price per square foot of the houses in our test set.\n",
    "Perhaps more variables could explain more variance. \n",
    "We can look again at what variables we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the variables in the data set\n",
    "X_train.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use all these variables to predict the price per square foot.\n",
    "Maybe the age of the house could also play a big influence.\n",
    "Therefore, we will now try a new linear regression with these 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "model_lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine which variables we pass to the model\n",
    "variables = ['grade','last_known_change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train the model\n",
    "model_lin_reg.fit(X_train[variables], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "print('adj. R^2:', (1-(1-model_lin_reg.score(X_test[variables], y_test))*(X_test.shape[0]- 1)/(X_test.shape[0]-len(variables)-1)).round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with this additional variable, 48% of the variance in the price per square foot could be explained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only looked at the linear relationships between the variables and the price. However, it is possible that the relationship is not linear, but rather quadratic. \n",
    "We can easily extend our model by squaring our variables. Thus, instead of:\n",
    "\n",
    "$price = b*(grade) + c$\n",
    "\n",
    "we can also use \n",
    "\n",
    "$price=a*(grade)^{2}+b*(grade)+c$\n",
    "\n",
    "can be obtained.\n",
    "This is a type of feature engineering. We will apply it to our complete data set and see if we can improve our model even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create only polynomial variables of second order (^2)\n",
    "poly = PolynomialFeatures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the Train and Test data\n",
    "X_train_poly = X_train.copy()\n",
    "X_test_poly = X_test.copy()\n",
    "\n",
    "# drop the id column\n",
    "X_train_poly = X_train_poly.drop(columns=['id'])\n",
    "X_test_poly = X_test_poly.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create new variables by calling poly\n",
    "X_train_sq = poly.fit_transform(X_train_poly)\n",
    "\n",
    "# We have to do the same for our test data, of course\n",
    "X_test_sq = poly.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "model_lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also train the model with squared variables\n",
    "model_lin_reg.fit(X_train_sq, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "print('adjusted R^2:', (1-(1-model_lin_reg.score(X_test_sq, y_test))*(X_test_sq.shape[0]- 1)/(X_test_sq.shape[0]-X_test_sq.shape[1]-1)).round(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the additional squared variables, we were able to improve our result a bit more.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the adjusted R^2 value we have a possibility to evaluate the quality of our model, but it may be worthwhile to have a look at the real errors of the model graphically. This may help to identify systematic errors.\n",
    "For ease of interpretation, we choose the percentage price difference between our forecast and the true values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a few outliers here. We can take a closer look at the highest one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "# In order to better analyse the errors of our model, we create a new dataframe with the\n",
    "# columns \"price\" (the real price), as well as the latitudes and longitudes\n",
    "y_predictions = model_lin_reg.predict(X_test_sq)\n",
    "df_error = pd.DataFrame(y_test)\n",
    "df_error['latitude'] = X_test['lat']\n",
    "df_error['longitude'] = X_test['long']\n",
    "df_error['id'] = X_test['id']\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add the predicted price as a column as well, we must first reset the index\n",
    "df_error.reset_index(inplace=True, drop=True)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can also add the predicted price as a column and calculate the difference\n",
    "df_error['price_prediction'] = y_predictions.round(2)\n",
    "df_error['price_difference'] = (df_error['price_prediction'] - df_error['price']).round(2)\n",
    "df_error['price_difference_procent'] = ((df_error['price_difference']/df_error['price'])*100).round(2)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df_error,\n",
    "                        lat=\"latitude\",\n",
    "                        lon=\"longitude\",\n",
    "                        hover_data=[\"price\", \"price_prediction\", 'id'],\n",
    "                        color='price_difference_procent',\n",
    "                        color_continuous_scale=['green', 'yellow', 'red'],\n",
    "                        zoom=7.7,\n",
    "                        height=400)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error[df_error['price_difference_procent']==df_error['price_difference_procent'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test['id']==9272202260]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to take a closer look at these houses. King County also provides very good information on this. On [this page](https://localscape.property/#kingcountyassessor/My-Property) you can search for houses by their ID and get both the neighbourhood on a map and a picture of the house.\n",
    "In the field at the top left, change the selection \"Address\" to \"Parcel ID\" and add the \"id\" of our outlier.\n",
    "\n",
    "Under \"Basic Property Characteristics\" and on the map under \"KC Aerial Images\" we see that there is no longer a house on this property. Therefore, our data is misleading and our model estimates a much higher price.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation and hyperparameter tuning of linear regression\n",
    "\n",
    "In addition to our variables, we have also passed the squared variables to our last linear model. So we have passed a lot of variables to our model. Some may have no effect on the price at all. However, models try to extract some information from all variables. This leads to random variance in the data also being recognised as a pattern. This phenomenon is called \"overfitting\" the model to the data.\n",
    "For each algorithm there are ways to prevent this overfitting.\n",
    "\n",
    "In the case of linear regression, we force the model not to use variables for forecasting. We \"regularise\" the model. But instead of us telling the model which variables not to use, we let the model learn which variables offer the least added value and remove those variables (in the case of linear regression, variables are no longer considered if the learned coefficient (b) is zero). How much we regularise is up to us.\n",
    "[Here](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net) you can find information on the ElasticNet model used.\n",
    "To find out how much we should regularize our model, we can test different values for the regularzation parameters and see which will give us the best model. For this purpose we can use [GridSearch](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) with [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify which hyperparameters we want to change:\n",
    "# alpha: specifies how much we regularise:\n",
    "param_grid = {'alpha':[0.1,.5,1,5,10,],\n",
    "             'l1_ratio': [1,0.5,0]}\n",
    "\n",
    "# We determine the model, there must be 2 round brackets behind the model name!\n",
    "elastic = ElasticNet(max_iter=50000, tol=0.2)\n",
    "\n",
    "# Passing the model to a so-called parameter grid with 5-fold cross-validation\n",
    "elastic_cv= GridSearchCV(elastic,param_grid,cv=5, verbose=True,n_jobs=-1)\n",
    "\n",
    "# We train the model and optimise it via GridSearch\n",
    "elastic_cv.fit(X_train_sq,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the best parameters found by the GridSearch.\n",
    "elastic_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model with the optimal hyperparameters\n",
    "elastic = ElasticNet(max_iter=50000, tol=0.2,**elastic_cv.best_params_)\n",
    "\n",
    "elastic.fit(X_train_sq, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at how well our model performs on the test data\n",
    "adj_r2 = (1-(1-elastic.score(X_test_sq, y_test))*(X_test_sq.shape[0]- 1)/(X_test_sq.shape[0]-(X_test_sq.shape[1]-sum(elastic.coef_== 0))-1)).round(2)\n",
    "print('adjusted R^2:',adj_r2 )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to further improve our model through hyperparameter tuning and regularisation. \n",
    "As mentioned above, regularisation eliminates variables from the forecast. This is done by giving the coefficients of the linear regression a value of zero. With the following code we can look at the learned coefficients (here only a section of the first 5 coefficients) and see that for the first variable a coefficient of zero was calculated. This variable was therefore removed by our regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the first 5 learned coefficients of linear regression\n",
    "elastic.coef_[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "# In order to better analyse the errors of our model, we create a new dataframe with the\n",
    "# columns \"price\" (the real price), as well as the latitudes and longitudes\n",
    "y_predictions = elastic.predict(X_test_sq)\n",
    "df_error = pd.DataFrame(y_test)\n",
    "df_error['latitude'] = X_test['lat']\n",
    "df_error['longitude'] = X_test['long']\n",
    "df_error['id'] = X_test['id']\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add the predicted price as a column as well, we must first reset the index\n",
    "df_error.reset_index(inplace=True, drop=True)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can also add the predicted price as a column and calculate the difference \n",
    "df_error['price_prediction'] = y_predictions.round(2)\n",
    "df_error['price_difference'] = (df_error['price_prediction'] - df_error['price']).round(2)\n",
    "df_error['price_difference_procent'] = ((df_error['price_difference']/df_error['price'])*100).round(2)\n",
    "df_error.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(df_error,\n",
    "                        lat=\"latitude\",\n",
    "                        lon=\"longitude\",\n",
    "                        hover_data=[\"price\", \"price_prediction\", 'id'],\n",
    "                        color='price_difference_procent',\n",
    "                        color_continuous_scale=['green', 'yellow', 'red'],\n",
    "                        zoom=7.7,\n",
    "                        height=400)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error[df_error['price_difference_procent']==df_error['price_difference_procent'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[X_test['id']==5111400086]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this house. King County also provides very good information about it. On [this page](https://localscape.property/#kingcountyassessor/My-Property) you can search for houses by their ID and get both the neighbourhood on a map and a picture of the house.\n",
    "In the box at the top left, they change the \"Address\" selection to \"Parcel ID\" and add the \"id\" of our outlier there.\n",
    "\n",
    "This house sold in 2018 for a price of  295,000 USD. But King County appraisers also assessed a higher price in 2014: under \"Historical Value\" they see that in 2014 the \"Total Assessed Value\" was 212,000 USD. So this house sold for about half the appraised price!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "We have now trained a model that can predict the price of a house in King County. We can now save this model using [skops](https://skops.readthedocs.io/en/stable/modules/classes.html#module-skops.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skops.io as sio\n",
    "\n",
    "with open('model/model.bin', 'wb') as f_out:\n",
    "    sio.dump(elastic, f_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Despite the outliers, we succeeded in creating a model that predicts prices with an accuracy of 76%. We found that the creation of new variables, but also the squaring of these variables and the regularisation of the model play an important role in the quality of the prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb7e7bd19e6b082abfa5136f7b755936d5cc9a5dcc8aebb1a765dc8cdd06fcd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
